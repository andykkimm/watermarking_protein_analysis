"""
Protein Watermarking for ProteinMPNN
Adapts token-specific watermarking from LLMs to protein sequence generation.

Based on: "Token-Specific Watermarking with Enhanced Detectability and
Semantic Coherence for Large Language Models" (Huo et al., ICML 2024)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from scipy.stats import norm
from typing import List, Tuple, Dict
import hashlib


# =============================================================================
# 1. GENERATOR NETWORKS
# =============================================================================

class GammaGenerator(nn.Module):
    """
    γ-generator: Generates token-specific splitting ratios.
    Takes previous amino acid embedding and outputs γ ∈ (0, 1).
    """
    def __init__(self, embedding_dim=128, hidden_dim=64):
        super(GammaGenerator, self).__init__()
        self.fc1 = nn.Linear(embedding_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 1)
        self.relu = nn.LeakyReLU(0.01)
        self.sigmoid = nn.Sigmoid()

    def forward(self, prev_aa_embedding):
        """
        Args:
            prev_aa_embedding: (batch_size, embedding_dim) from ProteinMPNN

        Returns:
            gamma: (batch_size, 1) splitting ratio in (0, 1)
        """
        x = self.relu(self.fc1(prev_aa_embedding))
        gamma = self.sigmoid(self.fc2(x))
        return gamma


class DeltaGenerator(nn.Module):
    """
    δ-generator: Generates token-specific watermark logits.
    Takes previous amino acid embedding and outputs δ ∈ ℝ+.
    """
    def __init__(self, embedding_dim=128, hidden_dim=64):
        super(DeltaGenerator, self).__init__()
        self.fc1 = nn.Linear(embedding_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 1)
        self.relu = nn.LeakyReLU(0.01)
        self.softplus = nn.Softplus()

    def forward(self, prev_aa_embedding):
        """
        Args:
            prev_aa_embedding: (batch_size, embedding_dim) from ProteinMPNN

        Returns:
            delta: (batch_size, 1) watermark logit in ℝ+
        """
        x = self.relu(self.fc1(prev_aa_embedding))
        delta = self.softplus(self.fc2(x))
        return delta


# =============================================================================
# 2. WATERMARK EMBEDDING (GENERATION)
# =============================================================================

class ProteinWatermarker:
    """
    Embeds watermarks into protein sequences generated by ProteinMPNN.
    """

    # 20 standard amino acids
    AMINO_ACIDS = list('ACDEFGHIKLMNPQRSTVWY')
    AA_TO_IDX = {aa: i for i, aa in enumerate(AMINO_ACIDS)}

    def __init__(self, gamma_generator, delta_generator, secret_key='default_key'):
        """
        Args:
            gamma_generator: Trained GammaGenerator network
            delta_generator: Trained DeltaGenerator network
            secret_key: Secret key for reproducible green list generation
        """
        self.gamma_gen = gamma_generator
        self.delta_gen = delta_generator
        self.secret_key = secret_key
        self.vocab_size = 20  # 20 amino acids

    def _hash_to_seed(self, prev_aa: str) -> int:
        """Generate reproducible random seed from previous amino acid."""
        combined = f"{prev_aa}_{self.secret_key}"
        hash_val = int(hashlib.sha256(combined.encode()).hexdigest(), 16)
        return hash_val % (2**32)

    def _split_vocabulary(self, gamma: float, seed: int) -> Tuple[List[int], List[int]]:
        """
        Split amino acid vocabulary into green and red lists using Gumbel-Softmax.

        Args:
            gamma: Splitting ratio
            seed: Random seed for reproducibility

        Returns:
            green_list: Indices of green amino acids
            red_list: Indices of red amino acids
        """
        rng = np.random.RandomState(seed)

        # Gumbel-Softmax sampling (Equation 1 from paper)
        tau = 0.1  # Temperature
        green_probs = []

        for _ in range(self.vocab_size):
            # Sample from Gumbel(0,1)
            g0 = -np.log(-np.log(rng.uniform(0, 1)))
            g1 = -np.log(-np.log(rng.uniform(0, 1)))

            # Gumbel-Softmax
            y_hat = np.exp((np.log(gamma) + g0) / tau) / (
                np.exp((np.log(gamma) + g0) / tau) +
                np.exp((np.log(1 - gamma) + g1) / tau)
            )
            green_probs.append(y_hat)

        # Soft assignment: use probability threshold
        green_list = [i for i, p in enumerate(green_probs) if p > 0.5]
        red_list = [i for i, p in enumerate(green_probs) if p <= 0.5]

        return green_list, red_list

    def generate_watermarked_sequence(
        self,
        proteinmpnn_model,
        structure,
        temperature=1.0,
        return_stats=False
    ):
        """
        Generate protein sequence with watermark.

        Args:
            proteinmpnn_model: ProteinMPNN model
            structure: Backbone structure (coordinates, etc.)
            temperature: Sampling temperature
            return_stats: If True, return gamma/delta values

        Returns:
            sequence: Watermarked protein sequence
            stats: (optional) Dict with gamma, delta, green_list per position
        """
        self.gamma_gen.eval()
        self.delta_gen.eval()

        sequence = []
        stats = {'gamma': [], 'delta': [], 'green_lists': []}

        # Get sequence length from structure
        seq_length = structure['length']

        with torch.no_grad():
            for pos in range(seq_length):
                # Get ProteinMPNN logits for this position
                logits = proteinmpnn_model.get_logits(structure, sequence)
                # Shape: (vocab_size=20,)

                if pos == 0:
                    # First position: no watermark (no previous amino acid)
                    probs = F.softmax(logits / temperature, dim=-1)
                    aa_idx = torch.multinomial(probs, 1).item()
                else:
                    # Get previous amino acid embedding
                    prev_aa = sequence[-1]
                    prev_aa_emb = proteinmpnn_model.get_aa_embedding(prev_aa)
                    # Shape: (embedding_dim,)

                    # Generate gamma and delta
                    gamma = self.gamma_gen(prev_aa_emb.unsqueeze(0)).item()
                    delta = self.delta_gen(prev_aa_emb.unsqueeze(0)).item()

                    # Split vocabulary
                    seed = self._hash_to_seed(prev_aa)
                    green_list, red_list = self._split_vocabulary(gamma, seed)

                    # Apply watermark: add delta to green amino acids
                    watermarked_logits = logits.clone()
                    for aa_idx in green_list:
                        watermarked_logits[aa_idx] += delta

                    # Sample from watermarked distribution
                    probs = F.softmax(watermarked_logits / temperature, dim=-1)
                    aa_idx = torch.multinomial(probs, 1).item()

                    if return_stats:
                        stats['gamma'].append(gamma)
                        stats['delta'].append(delta)
                        stats['green_lists'].append(green_list)

                # Convert index to amino acid
                aa = self.AMINO_ACIDS[aa_idx]
                sequence.append(aa)

        sequence_str = ''.join(sequence)

        if return_stats:
            return sequence_str, stats
        return sequence_str

    def detect_watermark(
        self,
        sequence: str,
        use_theoretical_threshold=True,
        fpr=0.01
    ) -> Dict:
        """
        Detect if a protein sequence contains a watermark.

        Args:
            sequence: Protein sequence to test
            use_theoretical_threshold: Use theoretical FPR threshold
            fpr: False positive rate for threshold

        Returns:
            Dict with z_score, p_value, is_watermarked
        """
        self.gamma_gen.eval()

        T = len(sequence)
        green_count = 0
        sum_gamma = 0
        sum_variance = 0

        # Mock embedding function (replace with actual ProteinMPNN embeddings)
        def get_aa_embedding(aa):
            # Placeholder: use random embeddings
            # In practice, get from ProteinMPNN
            return torch.randn(128)

        with torch.no_grad():
            for i in range(1, T):
                prev_aa = sequence[i-1]
                curr_aa = sequence[i]

                # Get gamma for this position
                prev_aa_emb = get_aa_embedding(prev_aa)
                gamma = self.gamma_gen(prev_aa_emb.unsqueeze(0)).item()

                # Recreate green list
                seed = self._hash_to_seed(prev_aa)
                green_list, _ = self._split_vocabulary(gamma, seed)

                # Check if current amino acid is green
                curr_aa_idx = self.AA_TO_IDX[curr_aa]
                if curr_aa_idx in green_list:
                    green_count += 1

                # Accumulate statistics
                sum_gamma += gamma
                sum_variance += gamma * (1 - gamma)

        # Calculate z-score (Equation 3 from paper)
        if sum_variance == 0:
            z_score = 0
        else:
            z_score = (green_count - sum_gamma) / np.sqrt(sum_variance)

        # Calculate p-value (one-sided test)
        p_value = 1 - norm.cdf(z_score)

        # Determine threshold
        if use_theoretical_threshold:
            # Theoretical threshold from normal distribution
            threshold = norm.ppf(1 - fpr)
        else:
            threshold = 2.33  # Approximately 1% FPR

        is_watermarked = z_score > threshold

        return {
            'z_score': z_score,
            'p_value': p_value,
            'is_watermarked': is_watermarked,
            'threshold': threshold,
            'green_count': green_count,
            'total_positions': T - 1
        }


# =============================================================================
# 3. TRAINING LOSSES
# =============================================================================

def detection_loss(gamma_values, delta_values, green_probs):
    """
    Detection loss: maximize z-score for detectability.
    Uses differentiable approximation (Equation 4 from paper).

    Args:
        gamma_values: List of gamma values for each position
        delta_values: List of delta values for each position
        green_probs: List of probabilities of selecting green AA at each position

    Returns:
        L_detection: Negative z-score (to minimize)
    """
    T = len(green_probs)

    # Sum of green probabilities
    sum_green_probs = sum(green_probs)

    # Sum of gammas
    sum_gamma = sum(gamma_values)

    # Variance
    sum_variance = sum([gamma * (1 - gamma) for gamma in gamma_values])

    # Differentiable z-score
    z_score_hat = (sum_green_probs - sum_gamma) / torch.sqrt(sum_variance + 1e-8)

    # Loss: negative z-score (we want to maximize z-score)
    L_detection = -z_score_hat

    return L_detection


def semantic_loss_esm(watermarked_seq, original_seq, esm_model):
    """
    Semantic loss using protein language model (ESM) embeddings.
    Similar to SimCSE in paper, but for proteins.

    Args:
        watermarked_seq: Watermarked protein sequence
        original_seq: Original sequence (without watermark)
        esm_model: Pre-trained ESM model

    Returns:
        L_semantic: Negative cosine similarity
    """
    # Get ESM embeddings
    with torch.no_grad():
        emb_watermarked = esm_model.encode(watermarked_seq)
        emb_original = esm_model.encode(original_seq)

    # Cosine similarity
    similarity = F.cosine_similarity(emb_watermarked, emb_original, dim=0)

    # Loss: negative similarity (want similarity close to 1)
    L_semantic = -similarity

    return L_semantic


# =============================================================================
# 4. MULTI-OBJECTIVE OPTIMIZATION (MGDA)
# =============================================================================

def compute_mgda_weight(grad_detection, grad_semantic):
    """
    Compute optimal weight for MGDA (Appendix C from paper).

    Args:
        grad_detection: Gradient from detection loss
        grad_semantic: Gradient from semantic loss

    Returns:
        lambda_star: Optimal weight in [0, 1]
    """
    # Flatten gradients
    g_D = torch.cat([g.flatten() for g in grad_detection])
    g_S = torch.cat([g.flatten() for g in grad_semantic])

    # Dot products
    g_D_dot_g_D = torch.dot(g_D, g_D)
    g_S_dot_g_S = torch.dot(g_S, g_S)
    g_D_dot_g_S = torch.dot(g_D, g_S)

    # Compute lambda* (closed form from paper)
    if g_D_dot_g_S >= g_D_dot_g_D:
        lambda_star = 1.0
    elif g_D_dot_g_S >= g_S_dot_g_S:
        lambda_star = 0.0
    else:
        numerator = (g_S - g_D).dot(g_S)
        denominator = torch.norm(g_D - g_S) ** 2
        lambda_star = (numerator / denominator).item()

    return lambda_star


def train_watermark_generators(
    proteinmpnn_model,
    train_structures,
    esm_model,
    embedding_dim=128,
    epochs=100,
    lr=1e-4,
    device='cuda'
):
    """
    Train gamma and delta generators using MGDA.

    Args:
        proteinmpnn_model: Pre-trained ProteinMPNN model
        train_structures: List of protein backbone structures
        esm_model: Pre-trained ESM model for semantic loss
        embedding_dim: Dimension of amino acid embeddings
        epochs: Number of training epochs
        lr: Learning rate
        device: 'cuda' or 'cpu'

    Returns:
        gamma_gen: Trained GammaGenerator
        delta_gen: Trained DeltaGenerator
    """
    # Initialize generators
    gamma_gen = GammaGenerator(embedding_dim=embedding_dim).to(device)
    delta_gen = DeltaGenerator(embedding_dim=embedding_dim).to(device)

    # Optimizer
    optimizer = torch.optim.Adam(
        list(gamma_gen.parameters()) + list(delta_gen.parameters()),
        lr=lr
    )

    proteinmpnn_model.eval()
    esm_model.eval()

    for epoch in range(epochs):
        epoch_loss_detection = 0
        epoch_loss_semantic = 0

        for structure in train_structures:
            optimizer.zero_grad()

            # Generate original sequence (no watermark)
            with torch.no_grad():
                original_seq = proteinmpnn_model.generate(structure)

            # Generate watermarked sequence
            watermarker = ProteinWatermarker(gamma_gen, delta_gen)
            watermarked_seq, stats = watermarker.generate_watermarked_sequence(
                proteinmpnn_model, structure, return_stats=True
            )

            # Compute detection loss
            L_D = detection_loss(
                stats['gamma'],
                stats['delta'],
                stats['green_probs']  # Would need to compute this
            )

            # Compute semantic loss
            L_S = semantic_loss_esm(watermarked_seq, original_seq, esm_model)

            # Compute gradients
            grad_D = torch.autograd.grad(L_D,
                                        list(gamma_gen.parameters()) + list(delta_gen.parameters()),
                                        retain_graph=True, create_graph=False)

            grad_S = torch.autograd.grad(L_S,
                                        list(gamma_gen.parameters()) + list(delta_gen.parameters()),
                                        retain_graph=False, create_graph=False)

            # MGDA: compute optimal weight
            lambda_star = compute_mgda_weight(grad_D, grad_S)

            # Combined gradient
            for i, (p, g_d, g_s) in enumerate(zip(
                list(gamma_gen.parameters()) + list(delta_gen.parameters()),
                grad_D,
                grad_S
            )):
                if p.grad is None:
                    p.grad = lambda_star * g_d + (1 - lambda_star) * g_s
                else:
                    p.grad += lambda_star * g_d + (1 - lambda_star) * g_s

            # Update parameters
            optimizer.step()

            epoch_loss_detection += L_D.item()
            epoch_loss_semantic += L_S.item()

        # Print progress
        print(f"Epoch {epoch+1}/{epochs} - "
              f"Detection Loss: {epoch_loss_detection/len(train_structures):.4f}, "
              f"Semantic Loss: {epoch_loss_semantic/len(train_structures):.4f}")

    return gamma_gen, delta_gen


# =============================================================================
# 5. EVALUATION
# =============================================================================

def evaluate_watermark(
    gamma_gen,
    delta_gen,
    proteinmpnn_model,
    test_structures,
    num_natural_seqs=100
):
    """
    Evaluate watermark detectability and protein quality.

    Args:
        gamma_gen: Trained GammaGenerator
        delta_gen: Trained DeltaGenerator
        proteinmpnn_model: ProteinMPNN model
        test_structures: Test protein structures
        num_natural_seqs: Number of natural sequences for FPR calculation

    Returns:
        results: Dict with TPR, FPR, z-scores, etc.
    """
    watermarker = ProteinWatermarker(gamma_gen, delta_gen)

    # Generate watermarked sequences
    watermarked_seqs = []
    for structure in test_structures:
        seq = watermarker.generate_watermarked_sequence(proteinmpnn_model, structure)
        watermarked_seqs.append(seq)

    # Detect watermarks
    detections = [watermarker.detect_watermark(seq) for seq in watermarked_seqs]

    # Calculate TPR
    tpr = sum([d['is_watermarked'] for d in detections]) / len(detections)

    # Generate natural sequences (for FPR)
    natural_seqs = []
    for structure in test_structures[:num_natural_seqs]:
        seq = proteinmpnn_model.generate(structure)  # No watermark
        natural_seqs.append(seq)

    # Detect on natural sequences
    natural_detections = [watermarker.detect_watermark(seq) for seq in natural_seqs]

    # Calculate FPR
    fpr = sum([d['is_watermarked'] for d in natural_detections]) / len(natural_detections)

    # Average z-scores
    avg_z_watermarked = np.mean([d['z_score'] for d in detections])
    avg_z_natural = np.mean([d['z_score'] for d in natural_detections])

    results = {
        'tpr': tpr,
        'fpr': fpr,
        'avg_z_watermarked': avg_z_watermarked,
        'avg_z_natural': avg_z_natural,
        'num_watermarked': len(watermarked_seqs),
        'num_natural': len(natural_seqs)
    }

    return results


# =============================================================================
# 6. EXAMPLE USAGE
# =============================================================================

if __name__ == "__main__":
    print("Protein Watermarking Implementation")
    print("=" * 60)

    # Initialize generators
    gamma_gen = GammaGenerator(embedding_dim=128)
    delta_gen = DeltaGenerator(embedding_dim=128)

    print("\n1. Generator Networks Initialized")
    print(f"   γ-generator parameters: {sum(p.numel() for p in gamma_gen.parameters())}")
    print(f"   δ-generator parameters: {sum(p.numel() for p in delta_gen.parameters())}")

    # Example: Create watermarker
    watermarker = ProteinWatermarker(gamma_gen, delta_gen, secret_key="my_secret")

    print("\n2. Watermarker Created")
    print(f"   Vocabulary size: {watermarker.vocab_size} amino acids")
    print(f"   Secret key: {watermarker.secret_key}")

    # Example detection on a dummy sequence
    dummy_sequence = "MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQDNLSGAEKAVQVKVKALPDAQFEVVHSLAKWKRQTLGQHDFSAGEGLYTHMKALRPDEDRLSLEVGCSRLAKAVGQPLRVGVVKDEEILKVTVGVRGDKWIVEAVPGDVRIVQVSMVRPCRVLLKPVEETDTVLATGGHYEEFFCRALAEAGALPLGAANVAKYSAAQVGQVTIVKDGPRIVRALKDQVVVSGLMKDAADQVQEMLKWL"

    detection_result = watermarker.detect_watermark(dummy_sequence)

    print("\n3. Example Detection on Dummy Sequence")
    print(f"   Sequence length: {len(dummy_sequence)}")
    print(f"   Z-score: {detection_result['z_score']:.4f}")
    print(f"   P-value: {detection_result['p_value']:.4f}")
    print(f"   Watermarked: {detection_result['is_watermarked']}")
    print(f"   Green count: {detection_result['green_count']}/{detection_result['total_positions']}")

    print("\n" + "=" * 60)
    print("✓ Implementation ready!")
    print("\nNext steps:")
    print("  1. Integrate with ProteinMPNN")
    print("  2. Load ESM model for semantic loss")
    print("  3. Prepare training data (protein structures)")
    print("  4. Train generators with MGDA")
    print("  5. Evaluate on test set")
